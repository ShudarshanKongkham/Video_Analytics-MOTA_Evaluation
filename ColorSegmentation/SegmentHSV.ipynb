{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30860\\1724282080.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Create a mask using the HSV range\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minRange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Apply the mask to the original frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "# Create a window\n",
    "cv2.namedWindow('Tracking')\n",
    "\n",
    "# Create trackbars for HSV values\n",
    "cv2.createTrackbar('Hue Min', 'Tracking', 0, 179, nothing)\n",
    "cv2.createTrackbar('Hue Max', 'Tracking', 179, 179, nothing)\n",
    "cv2.createTrackbar('Sat Min', 'Tracking', 0, 255, nothing)\n",
    "cv2.createTrackbar('Sat Max', 'Tracking', 255, 255, nothing)\n",
    "cv2.createTrackbar('Val Min', 'Tracking', 0, 255, nothing)\n",
    "cv2.createTrackbar('Val Max', 'Tracking', 255, 255, nothing)\n",
    "\n",
    "# Load a single image\n",
    "# frame = cv2.imread(\"image.jpg\")  # Replace with your image file path\n",
    "# cap = cv2.VideoCapture(\"yellowFish.mp4\") \n",
    "cap = cv2.VideoCapture(0) \n",
    "ret, frame = cap.read()\n",
    "# frame = cv2.resize(frame, (0, 0), fx=0.2, fy=0.2)  # Resizing if needed\n",
    "\n",
    "# Convert the frame to HSV color space\n",
    "hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "while True:\n",
    "    # Get current positions of trackbars\n",
    "    h_min = cv2.getTrackbarPos('Hue Min', 'Tracking')\n",
    "    h_max = cv2.getTrackbarPos('Hue Max', 'Tracking')\n",
    "    s_min = cv2.getTrackbarPos('Sat Min', 'Tracking')\n",
    "    s_max = cv2.getTrackbarPos('Sat Max', 'Tracking')\n",
    "    v_min = cv2.getTrackbarPos('Val Min', 'Tracking')\n",
    "    v_max = cv2.getTrackbarPos('Val Max', 'Tracking')\n",
    "\n",
    "    # Define the lower and upper bounds for the HSV values\n",
    "    lower_bound = np.array([h_min, s_min, v_min])\n",
    "    upper_bound = np.array([h_max, s_max, v_max])\n",
    "\n",
    "    # Create a mask using the HSV range\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Apply the mask to the original frame\n",
    "    result = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    # Display the original frame and the result\n",
    "    cv2.imshow('Original', frame)\n",
    "    cv2.imshow('Segmented', result)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"Required HSV values:\", h_min, h_max, s_min, s_max, v_min, v_max)\n",
    "        break\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required HSV values : 0 179 0 255 0 255\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "# Create a window\n",
    "cv2.namedWindow('Tracking')\n",
    "\n",
    "# Create trackbars for HSV values\n",
    "cv2.createTrackbar('Hue Min', 'Tracking', 0, 179, nothing)\n",
    "cv2.createTrackbar('Hue Max', 'Tracking', 179, 179, nothing)\n",
    "cv2.createTrackbar('Sat Min', 'Tracking', 0, 255, nothing)\n",
    "cv2.createTrackbar('Sat Max', 'Tracking', 255, 255, nothing)\n",
    "cv2.createTrackbar('Val Min', 'Tracking', 0, 255, nothing)\n",
    "cv2.createTrackbar('Val Max', 'Tracking', 255, 255, nothing)\n",
    "\n",
    "# Load an image or use video capture\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera or provide a filename\n",
    "# cap = cv2.VideoCapture(\"yellowFish.mp4\")  # Use 0 for the default camera or provide a filename\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        # cap = cv2.VideoCapture(\"yellowFish.mp4\") \n",
    "        ret, frame = cap.read()\n",
    "        continue\n",
    "    # frame = cv2.resize(frame, (0, 0), fx = 0.2, fy = 0.2)\n",
    "\n",
    "\n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Get current positions of trackbars\n",
    "    h_min = cv2.getTrackbarPos('Hue Min', 'Tracking')\n",
    "    h_max = cv2.getTrackbarPos('Hue Max', 'Tracking')\n",
    "    s_min = cv2.getTrackbarPos('Sat Min', 'Tracking')\n",
    "    s_max = cv2.getTrackbarPos('Sat Max', 'Tracking')\n",
    "    v_min = cv2.getTrackbarPos('Val Min', 'Tracking')\n",
    "    v_max = cv2.getTrackbarPos('Val Max', 'Tracking')\n",
    "\n",
    "    # Define the lower and upper bounds for the HSV values\n",
    "    lower_bound = np.array([h_min, s_min, v_min])\n",
    "    upper_bound = np.array([h_max, s_max, v_max])\n",
    "\n",
    "    # Create a mask using the HSV range\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Apply the mask to the original frame\n",
    "    result = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    # Display the original frame and the result\n",
    "    cv2.imshow('Original', frame)\n",
    "    cv2.imshow('Segmented', result)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"Required HSV values :\",h_min,h_max, s_min, s_max, v_min, v_max)\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hue min: 18\n",
    "hue max: 169\n",
    "sat min: 129\n",
    "sat max: 183\n",
    "val min: 49\n",
    "val max: 253\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required HSV values: 23 69 80 255 107 255\n",
    "Required HSV values : 146 179 30 148 178 251 ##pink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['color_segmentation.py',\n",
       " 'imageFeatures.ipynb',\n",
       " 'image_data',\n",
       " 'Image_Processing.ipynb',\n",
       " 'SegmentHSV.ipynb',\n",
       " 'video_data']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Fixed HSV values for segmentation\n",
    "hue_min, hue_max = 23, 69\n",
    "sat_min, sat_max = 80, 255\n",
    "val_min, val_max = 107, 255\n",
    "\n",
    "# hue_min, hue_max = 146, 179\n",
    "# sat_min, sat_max = 30, 148\n",
    "# val_min, val_max = 178, 251\n",
    "\n",
    "# Load an image or use video capture\n",
    "# cap = cv2.VideoCapture(0)  # Use 0 for the default camera or provide a filename\n",
    "cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")  # Use 0 for the default camera or provide a filename\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    # print(\"YO\", ret)\n",
    "\n",
    "    if not ret:\n",
    "        cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")  # reinitialize video\n",
    "        # cap = cv2.VideoCapture(0)\n",
    "        ret, frame = cap.read()\n",
    "        break\n",
    "    # print(\"YO\")\n",
    "    frame = cv2.resize(frame, (0, 0), fx = 0.3, fy = 0.3)\n",
    "\n",
    "\n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the lower and upper bounds for the HSV values\n",
    "    lower_bound = np.array([hue_min, sat_min, val_min])\n",
    "    upper_bound = np.array([hue_max, sat_max, val_max])\n",
    "\n",
    "    # Create a mask using the HSV range\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create a copy of the frame to draw on\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # Draw bounding boxes around detected objects\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Filter out small contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw green bounding box\n",
    "\n",
    "            # Optionally, fill the detected region with a color\n",
    "            cv2.drawContours(output_frame, [contour], -1, (0, 0, 255), -1)  # Fill with green color\n",
    "\n",
    "    # Display the original frame and the result with bounding boxes\n",
    "    cv2.imshow('Original', frame)\n",
    "    cv2.imshow('MASK', mask)\n",
    "    cv2.imshow('Segmented with Bounding Boxes', output_frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use optical flow on the segmented object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Optical Flow\n",
    "\n",
    "## What is Optical Flow?\n",
    "\n",
    "Optical flow refers to the pattern of apparent motion of objects or surfaces in a visual scene, caused by the relative motion between an observer and the scene. It is a fundamental concept in computer vision and image processing, used to analyze and understand the motion within a sequence of images or video frames.\n",
    "\n",
    "In essence, optical flow is used to estimate the velocity of objects or pixels in a video sequence, providing a dense motion field that describes how each pixel moves from one frame to another.\n",
    "\n",
    "## How Optical Flow Works\n",
    "\n",
    "### Basic Principles\n",
    "\n",
    "Optical flow is based on the assumption that the intensity of objects in a video frame remains constant as they move. This principle is known as the **brightness constancy assumption**. The core idea is that the observed changes in the image intensity between consecutive frames are due to the motion of objects in the scene.\n",
    "\n",
    "### The Optical Flow Equation\n",
    "\n",
    "The optical flow can be described mathematically by the following equation, derived from the brightness constancy assumption:\n",
    "\n",
    "\\[ I(x, y, t) = I(x + u \\Delta t, y + v \\Delta t, t + \\Delta t) \\]\n",
    "\n",
    "Where:\n",
    "- \\( I(x, y, t) \\) is the image intensity at position \\((x, y)\\) and time \\(t\\).\n",
    "- \\(u\\) and \\(v\\) are the horizontal and vertical components of the optical flow velocity.\n",
    "- \\(\\Delta t\\) is the time difference between frames.\n",
    "\n",
    "By approximating the intensity change, we can derive the optical flow equation:\n",
    "\n",
    "\\[ I_x u + I_y v + I_t = 0 \\]\n",
    "\n",
    "Where:\n",
    "- \\(I_x\\) and \\(I_y\\) are the spatial gradients of the image intensity.\n",
    "- \\(I_t\\) is the temporal gradient of the image intensity.\n",
    "\n",
    "### Computing Optical Flow\n",
    "\n",
    "There are several methods to compute optical flow, but the two most commonly used algorithms are:\n",
    "\n",
    "1. **Lucas-Kanade Method**:\n",
    "   - This method assumes that the flow is essentially constant in a local neighborhood of the pixel under consideration.\n",
    "   - It computes the flow vectors by solving a set of linear equations derived from the optical flow equation, usually using least squares.\n",
    "   - It is suitable for small, local motions and is widely used for real-time applications.\n",
    "\n",
    "2. **Horn-Schunck Method**:\n",
    "   - This method is based on global smoothness constraints, assuming that the flow field is smooth and continuous over the entire image.\n",
    "   - It uses an energy minimization approach to find the flow vectors that satisfy both the optical flow equation and the smoothness constraint.\n",
    "   - It is more computationally intensive compared to Lucas-Kanade but can handle larger and more complex motions.\n",
    "\n",
    "### Applications of Optical Flow\n",
    "\n",
    "Optical flow has a wide range of applications in computer vision and image processing, including:\n",
    "\n",
    "- **Motion Detection**: Identifying moving objects within a video sequence.\n",
    "- **Object Tracking**: Following the trajectory of objects across multiple frames.\n",
    "- **Video Stabilization**: Reducing the effects of camera shake by compensating for motion.\n",
    "- **Scene Reconstruction**: Creating 3D models of scenes by analyzing motion patterns.\n",
    "- **Activity Recognition**: Understanding human actions and behaviors by analyzing motion.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Optical flow is a powerful tool for understanding motion in video sequences. By estimating the velocity of pixels between frames, it allows for various advanced applications in computer vision. Different methods, such as Lucas-Kanade and Horn-Schunck, provide solutions to compute optical flow based on the assumptions of motion and intensity changes. Whether used for tracking objects, detecting motion, or stabilizing video, optical flow remains a crucial concept in the field of image processing and computer vision.\n",
    "\n",
    "Feel free to ask more questions or request further explanations on specific aspects of optical flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Fixed HSV values for segmentation\n",
    "hue_min, hue_max = 23, 69\n",
    "sat_min, sat_max = 80, 255\n",
    "val_min, val_max = 107, 255\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(\"video_data/scisors.mp4\")  # Use the video file\n",
    "\n",
    "# Variables for optical flow\n",
    "prev_gray = None\n",
    "prev_centroids = []\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        # Reinitialize video capture if the end of video is reached\n",
    "        cap = cv2.VideoCapture(\"video_data/scisors.mp4\")  # Reinitialize video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (0, 0), fx=0.3, fy=0.3)\n",
    "\n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the lower and upper bounds for the HSV values\n",
    "    lower_bound = np.array([hue_min, sat_min, val_min])\n",
    "    upper_bound = np.array([hue_max, sat_max, val_max])\n",
    "\n",
    "    # Create a mask using the HSV range\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create a copy of the frame to draw on\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # Initialize variables for optical flow\n",
    "    new_prev_centroids = []\n",
    "\n",
    "    # Draw bounding boxes around detected objects and track centroid\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Filter out small contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw green bounding box\n",
    "\n",
    "            # Calculate the centroid of the bounding box\n",
    "            cx = x + w // 2\n",
    "            cy = y + h // 2\n",
    "            new_prev_centroids.append(np.array([cx, cy], dtype=np.float32))\n",
    "\n",
    "            # Draw centroid\n",
    "            cv2.circle(output_frame, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for centroid\n",
    "\n",
    "    # Update optical flow tracking\n",
    "    if prev_gray is not None and len(prev_centroids) > 0 and len(new_prev_centroids) > 0:\n",
    "        # Calculate optical flow using Lucas-Kanade method\n",
    "        prev_pts = np.array(prev_centroids, dtype=np.float32)\n",
    "        new_pts, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), prev_pts, None)\n",
    "\n",
    "        # Check if optical flow computation was successful\n",
    "        if new_pts is not None and status is not None:\n",
    "            # Select good points\n",
    "            status = status.reshape(-1)  # Flatten the status array\n",
    "            good_new = new_pts[status == 1]\n",
    "            good_old = np.array(prev_centroids)[status == 1]\n",
    "\n",
    "            # Draw tracks\n",
    "            for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                a, b = new.ravel()\n",
    "                c, d = old.ravel()\n",
    "                cv2.line(output_frame, (int(a), int(b)), (int(c), int(d)), color=[0, 255, 0], thickness=2)\n",
    "                cv2.circle(output_frame, (int(a), int(b)), 5, color=[0, 0, 255], thickness=-1)\n",
    "\n",
    "    # Update previous frame and centroids\n",
    "    prev_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    prev_centroids = new_prev_centroids\n",
    "\n",
    "    # Display the frame with bounding boxes, centroids, and path\n",
    "    cv2.imshow('Segmented with Bounding Boxes and Optical Flow', output_frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach in the updated code primarily tracks the path of centroids of detected contours, which is conceptually different from traditional optical flow tracking. Here's a comparison to help clarify the differences:\n",
    "\n",
    "### **Optical Flow:**\n",
    "\n",
    "1. **Concept:**\n",
    "   - Optical flow is a technique used to estimate the motion of objects between consecutive frames in a video. It calculates the apparent motion of pixel intensity patterns and is used to track movement at a fine-grained level.\n",
    "\n",
    "2. **How It Works:**\n",
    "   - It uses algorithms like the Lucas-Kanade method or the Horn-Schunck method to compute motion vectors for every pixel or a subset of pixels (feature points) between frames.\n",
    "   - Optical flow computes the displacement of these feature points, providing a dense or sparse representation of motion.\n",
    "\n",
    "3. **Applications:**\n",
    "   - Tracking objects at a pixel level.\n",
    "   - Motion detection.\n",
    "   - Video stabilization.\n",
    "   - Object tracking in complex scenes.\n",
    "\n",
    "4. **Implementation:**\n",
    "   - Optical flow algorithms analyze the changes in pixel intensities over time to compute motion vectors.\n",
    "   - It is often used with feature points or keypoints to track specific elements within the scene.\n",
    "\n",
    "### **Path Tracking Using Contour Centroids:**\n",
    "\n",
    "1. **Concept:**\n",
    "   - This method tracks the centroid of detected contours or bounding boxes from frame to frame. It essentially traces the path of the central point of the detected object, rather than the detailed motion of individual pixels.\n",
    "\n",
    "2. **How It Works:**\n",
    "   - After detecting contours and calculating their bounding boxes, the centroid of each bounding box is computed.\n",
    "   - The path of these centroids is then tracked and drawn over the frames.\n",
    "\n",
    "3. **Applications:**\n",
    "   - Monitoring the trajectory of detected objects.\n",
    "   - Visualizing movement of larger objects or regions of interest.\n",
    "   - Simplified tracking when detailed motion analysis is not required.\n",
    "\n",
    "4. **Implementation:**\n",
    "   - This method does not compute motion vectors or pixel-level displacement but rather tracks and visualizes the central point of detected objects.\n",
    "   - It is generally simpler and faster but less precise than optical flow.\n",
    "\n",
    "### **Comparison:**\n",
    "\n",
    "1. **Detail Level:**\n",
    "   - **Optical Flow:** Provides detailed motion information at the pixel level or for selected feature points.\n",
    "   - **Path Tracking with Centroids:** Tracks the movement of object centroids, providing less detailed motion information but useful for visualizing object trajectories.\n",
    "\n",
    "2. **Complexity:**\n",
    "   - **Optical Flow:** More complex and computationally intensive. Requires handling pixel intensities and calculating motion vectors.\n",
    "   - **Path Tracking with Centroids:** Simpler and computationally less demanding. Involves detecting contours, computing centroids, and drawing paths.\n",
    "\n",
    "3. **Applications:**\n",
    "   - **Optical Flow:** Suitable for applications needing precise motion tracking and analysis.\n",
    "   - **Path Tracking with Centroids:** Suitable for applications where tracking object trajectories or simplifying tracking of larger objects is sufficient.\n",
    "\n",
    "In summary, while optical flow provides detailed motion tracking at a fine-grained level, tracking the path of centroids is a higher-level approach that simplifies the problem by focusing on the central points of detected objects. Each method has its own strengths and is chosen based on the specific requirements of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace the path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Tracking Process Using Centroids\n",
    "\n",
    "Path tracking using centroids involves a systematic approach to monitor and visualize the movement of objects within a video stream by focusing on their central points. The process begins with the detection of objects in each frame, typically achieved through segmentation techniques that isolate objects based on specific color or intensity thresholds. Once the objects are detected, their centroids, which are the geometric centers of their bounding boxes, are calculated. The centroid provides a single point of reference for each detected object, simplifying the tracking of its movement.\n",
    "\n",
    "To track the path of these centroids over time, the centroids from each frame are recorded and stored in a list or history. This historical data captures the trajectory of the centroids as the video progresses. As new frames are processed, the centroids of the detected objects are appended to this list, creating a chronological sequence of their positions. Visualization is achieved by drawing lines or paths that connect consecutive centroid positions, thus illustrating the trajectory of each object. This method allows for the clear representation of the movement patterns of the objects, providing valuable insights into their paths and behavior throughout the video sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Color Segmentation and Centroid Tracking)\n",
    "> Color Segmentation: Uses fixed HSV values to create a mask that highlights objects of a specific color.\n",
    "\n",
    "> Contour Detection: Finds contours in the mask to identify objects.\n",
    "\n",
    "> Centroid Calculation: Calculates the centroid of each detected object.\n",
    "\n",
    "> Path Tracking: Tracks the centroids over time based on their proximity to previous positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Fixed HSV values for segmentation\n",
    "hue_min, hue_max = 23, 69\n",
    "sat_min, sat_max = 80, 255\n",
    "val_min, val_max = 107, 255\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")  # Use the video file\n",
    "\n",
    "# List to store the path history of centroids\n",
    "paths = []\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        # Reinitialize video capture if the end of video is reached\n",
    "        cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")  # Reinitialize video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (0, 0), fx=0.3, fy=0.3)\n",
    "\n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the lower and upper bounds for the HSV values\n",
    "    lower_bound = np.array([hue_min, sat_min, val_min])\n",
    "    upper_bound = np.array([hue_max, sat_max, val_max])\n",
    "\n",
    "    # Create a mask using the HSV range\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create a copy of the frame to draw on\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # Initialize new centroids list\n",
    "    new_centroids = []\n",
    "\n",
    "    # Draw bounding boxes around detected objects and track centroid\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Filter out small contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw green bounding box\n",
    "\n",
    "            # Calculate the centroid of the bounding box\n",
    "            cx = x + w // 2\n",
    "            cy = y + h // 2\n",
    "            new_centroids.append((cx, cy))\n",
    "\n",
    "            # Draw centroid\n",
    "            cv2.circle(output_frame, (cx, cy), 5, (0, 255, 0), -1)  # Red circle for centroid\n",
    "\n",
    "    # Update the path history with new centroids\n",
    "    if len(new_centroids) > 0:\n",
    "        # Append new centroids to paths\n",
    "        for centroid in new_centroids:\n",
    "            # Check if the path for this centroid already exists\n",
    "            found = False\n",
    "            for path in paths:\n",
    "                if len(path) > 0 and np.linalg.norm(np.array(path[-1]) - np.array(centroid)) < 50:\n",
    "                    path.append(centroid)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                paths.append([centroid])\n",
    "\n",
    "    # Keep only the last 10 points in each path\n",
    "    for path in paths:\n",
    "        if len(path) > 25:\n",
    "            del path[:-25]\n",
    "\n",
    "    # Draw paths\n",
    "    for path in paths:\n",
    "        if len(path) > 1:\n",
    "            for i in range(len(path) - 1):\n",
    "                cv2.line(output_frame, path[i], path[i + 1], color=(0, 0, 255), thickness=2)\n",
    "\n",
    "    # Debug: Show mask to ensure proper segmentation\n",
    "    cv2.imshow('Mask', mask)\n",
    "    \n",
    "    # Display the frame with bounding boxes, centroids, and path\n",
    "    cv2.imshow('Segmented with Bounding Boxes and Path Tracking', output_frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical FLOW (Using Farneback Method)\n",
    "\n",
    "Optical flow is a technique used to estimate the motion of objects between consecutive frames in a video. It calculates the apparent motion of brightness patterns in the image, which can be used to track objects more accurately, especially when they are moving in complex ways. Hereâ€™s a comparison between your current approach and optical flow:\n",
    "\n",
    "Motion Estimation: Estimates the motion of pixels between consecutive frames.\n",
    "Tracking: Tracks the movement of objects by following the flow vectors.\n",
    "Accuracy: Can handle complex motions and partial occlusions better than simple centroid tracking.\n",
    "Methods:\n",
    "Lucas-Kanade Method: Computes optical flow for a sparse set of feature points.\n",
    "Farneback Method: Computes dense optical flow for all pixels in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(\"video_data/runner.mp4\")\n",
    "# cap = cv2.VideoCapture(\"video_data/bmx.mp4\")\n",
    "\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "frame1 = cv2.resize(frame1, (0, 0), fx=0.3, fy=0.3)\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        cap = cv2.VideoCapture(\"video_data/runner.mp4\")\n",
    "        # cap = cv2.VideoCapture(\"video_data/bmx.mp4\")\n",
    "        ret, frame2 = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    frame2 = cv2.resize(frame2, (0, 0), fx=0.3, fy=0.3)\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate dense optical flow using Farneback method\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Compute the magnitude and angle of the flow vectors\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv = np.zeros_like(frame1)\n",
    "    hsv[..., 1] = 255\n",
    "    hsv[..., 0] = angle * 180 / np.pi / 2\n",
    "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Display the optical flow\n",
    "    cv2.imshow('Optical Flow', rgb)\n",
    "\n",
    "    # Update the previous frame\n",
    "    prvs = next\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Flow (Lucas-Kanade)\n",
    "Explanation:\n",
    "> Feature Detection: The code uses the Shi-Tomasi method to detect good features to track in the first frame.\n",
    "\n",
    "> Optical Flow Calculation: The Lucas-Kanade method is used to calculate the optical flow for these features in subsequent frames.\n",
    "\n",
    "> Drawing Tracks: The tracks of the detected features are drawn on the frames to visualize their movement.\n",
    "\n",
    "This method is particularly useful for tracking specific points or features in a video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")\n",
    "\n",
    "# Parameters for Shi-Tomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Create some random colors for drawing\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "# Take the first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "old_frame = cv2.resize(old_frame, (0, 0), fx=0.3, fy=0.3)\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    frame = cv2.resize(frame, (0, 0), fx=0.3, fy=0.3)\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "    # Select good points\n",
    "    good_new = p1[st == 1]\n",
    "    good_old = p0[st == 1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel().astype(int)\n",
    "        c, d = old.ravel().astype(int)\n",
    "        mask = cv2.line(mask, (a, b), (c, d), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame, (a, b), 5, color[i].tolist(), -1)\n",
    "\n",
    "    img = cv2.add(frame, mask)\n",
    "\n",
    "    # Display the frame with optical flow tracks\n",
    "    cv2.imshow('Optical Flow - Lucas-Kanade', img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLOR SEGMENTATION + OPTICAL FLOW\n",
    "Explanation:\n",
    "> Bounding Box Detection: The code first detects objects using color segmentation and draws bounding boxes around them.\n",
    "\n",
    "> Corner Detection within Bounding Boxes: It then detects good features to track within each bounding box.\n",
    "\n",
    "> Optical Flow Calculation: The Lucas-Kanade method is used to track these features across frames.\n",
    "\n",
    "> Drawing Tracks: The tracks of the detected features are drawn on the frames to visualize their movement.\n",
    "\n",
    "This approach ensures that optical flow is applied only within the detected bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Fixed HSV values for segmentation\n",
    "hue_min, hue_max = 23, 69\n",
    "sat_min, sat_max = 80, 255\n",
    "val_min, val_max = 107, 255\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")\n",
    "\n",
    "# Parameters for Shi-Tomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Create some random colors for drawing\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "# Initialize variables\n",
    "old_gray = None\n",
    "p0 = None\n",
    "mask = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    frame = cv2.resize(frame, (0, 0), fx=0.3, fy=0.3)\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the lower and upper bounds for the HSV values\n",
    "    lower_bound = np.array([hue_min, sat_min, val_min])\n",
    "    upper_bound = np.array([hue_max, sat_max, val_max])\n",
    "\n",
    "    # Create a mask using the HSV range\n",
    "    mask_hsv = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask_hsv, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create a copy of the frame to draw on\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # Initialize new points to track\n",
    "    new_points = []\n",
    "\n",
    "    # Draw bounding boxes around detected objects\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Filter out small contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw green bounding box\n",
    "\n",
    "            # Define the region of interest (ROI) for corner detection\n",
    "            roi_gray = frame_gray[y:y+h, x:x+w]\n",
    "            roi_color = output_frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Detect corners in the ROI\n",
    "            p = cv2.goodFeaturesToTrack(roi_gray, mask=None, **feature_params)\n",
    "            if p is not None:\n",
    "                p[:, 0, 0] += x\n",
    "                p[:, 0, 1] += y\n",
    "                new_points.append(p)\n",
    "\n",
    "    if len(new_points) > 0:\n",
    "        p0 = np.vstack(new_points)\n",
    "\n",
    "    if old_gray is None:\n",
    "        old_gray = frame_gray.copy()\n",
    "        mask = np.zeros_like(output_frame)\n",
    "        continue\n",
    "\n",
    "    if p0 is not None:\n",
    "        # Calculate optical flow\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "        # Select good points\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "\n",
    "        # Draw the tracks\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new.ravel().astype(int)\n",
    "            c, d = old.ravel().astype(int)\n",
    "            mask = cv2.line(mask, (a, b), (c, d), color[i].tolist(), 2)\n",
    "            output_frame = cv2.circle(output_frame, (a, b), 5, color[i].tolist(), -1)\n",
    "\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    img = cv2.add(output_frame, mask)\n",
    "\n",
    "    # Display the original frame and the result with bounding boxes and optical flow\n",
    "    cv2.imshow('Original', frame)\n",
    "    cv2.imshow('MASK', mask_hsv)\n",
    "    cv2.imshow('Segmented with Bounding Boxes and Optical Flow', img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    old_gray = frame_gray.copy()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Fixed HSV values for segmentation\n",
    "hue_min, hue_max = 23, 69\n",
    "sat_min, sat_max = 80, 255\n",
    "val_min, val_max = 107, 255\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")  # Use the video file\n",
    "\n",
    "# List to store the path history of centroids\n",
    "paths = []\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        # Reinitialize video capture if the end of video is reached\n",
    "        cap = cv2.VideoCapture(\"video_data/yellowFish.mp4\")  # Reinitialize video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (0, 0), fx=0.3, fy=0.3)\n",
    "\n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the lower and upper bounds for the HSV values\n",
    "    lower_bound = np.array([hue_min, sat_min, val_min])\n",
    "    upper_bound = np.array([hue_max, sat_max, val_max])\n",
    "\n",
    "    # Create a mask using the HSV range\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create a copy of the frame to draw on\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # Initialize new centroids list\n",
    "    new_centroids = []\n",
    "\n",
    "    # Draw bounding boxes around detected objects and track centroid\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Filter out small contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw green bounding box\n",
    "\n",
    "            # Calculate the centroid of the bounding box\n",
    "            cx = x + w // 2\n",
    "            cy = y + h // 2\n",
    "            new_centroids.append(np.array([cx, cy], dtype=np.float32))\n",
    "\n",
    "            # Draw centroid\n",
    "            cv2.circle(output_frame, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for centroid\n",
    "\n",
    "    # Update the path history with new centroids\n",
    "    if len(new_centroids) > 0:\n",
    "        paths.append(new_centroids)\n",
    "\n",
    "    # Draw paths\n",
    "    for path in paths:\n",
    "        if len(path) > 1:\n",
    "            for i in range(len(path) - 1):\n",
    "                cv2.line(output_frame, tuple(path[i].astype(int)), tuple(path[i + 1].astype(int)), color=(255, 0, 0), thickness=2)\n",
    "\n",
    "    # Debug: Show mask to ensure proper segmentation\n",
    "    cv2.imshow('Mask', mask)\n",
    "    \n",
    "    # Display the frame with bounding boxes, centroids, and path\n",
    "    cv2.imshow('Segmented with Bounding Boxes and Path Tracking', output_frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Fixed HSV values for segmentation\n",
    "hue_min, hue_max = 23, 69\n",
    "sat_min, sat_max = 80, 255\n",
    "val_min, val_max = 107, 255\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(\"yellowFish.mp4\")  # Use the video file\n",
    "\n",
    "# List to store the path history of centroids\n",
    "paths = []\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        # Reinitialize video capture if the end of video is reached\n",
    "        cap = cv2.VideoCapture(\"yellowFish.mp4\")  # Reinitialize video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (0, 0), fx=0.3, fy=0.3)\n",
    "\n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the lower and upper bounds for the HSV values\n",
    "    lower_bound = np.array([hue_min, sat_min, val_min])\n",
    "    upper_bound = np.array([hue_max, sat_max, val_max])\n",
    "\n",
    "    # Create a mask using the HSV range\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create a copy of the frame to draw on\n",
    "    output_frame = frame.copy()\n",
    "\n",
    "    # Initialize new centroids list\n",
    "    new_centroids = []\n",
    "\n",
    "    # Draw bounding boxes around detected objects and track centroid\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Filter out small contours\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw green bounding box\n",
    "\n",
    "            # Calculate the centroid of the bounding box\n",
    "            cx = x + w // 2\n",
    "            cy = y + h // 2\n",
    "            new_centroids.append(np.array([cx, cy], dtype=np.float32))\n",
    "\n",
    "            # Draw centroid\n",
    "            cv2.circle(output_frame, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for centroid\n",
    "\n",
    "    # Update the path history with new centroids\n",
    "    if len(new_centroids) > 0:\n",
    "        paths.append(new_centroids)\n",
    "\n",
    "    # Draw paths\n",
    "    for path in paths:\n",
    "        if len(path) > 1:\n",
    "            # Draw lines connecting consecutive centroids\n",
    "            for i in range(len(path) - 1):\n",
    "                cv2.line(output_frame, tuple(path[i].astype(int)), tuple(path[i + 1].astype(int)), color=(255, 0, 0), thickness=2)\n",
    "\n",
    "    # Display the frame with bounding boxes, centroids, and path\n",
    "    cv2.imshow('Segmented with Bounding Boxes and Path Tracking', output_frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizin A #D plot in TKINTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"3D Plot in Tkinter\")\n",
    "\n",
    "# Create a figure\n",
    "fig = Figure(figsize=(5, 4), dpi=100)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Generate some 3D data\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = np.sin(np.sqrt(x**2 + y**2))\n",
    "\n",
    "# Plot the data\n",
    "ax.plot_surface(x, y, z, cmap='viridis')\n",
    "\n",
    "# Create a canvas and add the figure to it\n",
    "canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "canvas.draw()\n",
    "canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "tk.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
